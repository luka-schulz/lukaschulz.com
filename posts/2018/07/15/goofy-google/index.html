<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport"content="initial-scale=1">

    <title>Machine Learning</title>

    <meta property="og:title" content="Blog"/>
    <meta property="og:type" content="website" />
    <meta property="og:site_name" content="uka Schulz, Ordinaire"/>
    <meta property="og:url" content="http://lukaschulz.com/blog"/>
    <meta property="og:description" content="{{ page.content | strip_html | lstrip | truncatewords: 50 }}"/>
    <meta property="og:image" content=""/>

    <meta name="theme-color" content="#7b7b7b" />

    <link href="../../../../../css/style.css" rel="stylesheet" type="text/css">

    <link rel="shortcut icon" href="../../../../../yes-its-upsidedown.png">

  </head>

  <body class="phl">

    <header class="wrap sans site__header"role="banner">
      <a class="site__title b plainlink"href="../../../../../index.html">Luka Schulz</a>

      <nav class="site__nav"role="navigation">
        <ul class="nav">
          <li><a class="plainlink"href="../../../../../blog.html">Blog</a></li>
          <li><a class="plainlink"href="../../../../../about.html">About</a></li>

        </ul>
      </nav>

    </header>

    <main class="wrap site__content" role="main">
      
      <header class="page__header mbl">
        <h1>Machine Learning: An Interaction Odessy</h1>
        <p class="post__date small">Sun. July, 17 2018</p>
      </header>

      <article class="post__content">
        <figure>
          <a href="./ciid-mantra.jpg" title="Copenhagen Institute of Interaction Design Mantra: listen, Build, test, repeat." target="_blank">
            <img src="./ciid-mantra.jpg" style="background: rgb(211, 211, 211);">
          </a>
          <figcaption>
            <p class="small">Photo credit: Ana Laura Farias</p>
          </figcaption>
        </figure>
        
        <p>I had the pleasure of taking part in the 2018 Machine Learning for Interaction Design summer school program hosted by the Copenhagen Institute of Interaction Design (CIID). The class was taught by <a href="https://andreasrefsgaard.dk/">Andreas Refsgaard</a> and <a href="http://genekogan.com/">Gene Kogan</a> at UN City in Copenhagen and lasted one week (July 9 – 13, 2018). During that period, they previewed numerous machine learning tools and programs that could be utilized by interaction designers.</p>
        
        <h2 class="mb">Day 1</h2>
        
        <p>On the first day of the class we were introduced to <a href="http://www.wekinator.org/">Wekinator</a> and <a href="https://ml5js.org/">ml5.js</a>, the former being a client-side application and the latter a JavaScript library. In addition to these two machine learning tools, we were also introduced to Processing, p5.js, and openFrameworks. With these tools under our belt, several lectures, and a helping of Danish dining, we spent some time playing around with Wekintator and Processing, using them to prototype a Yoga Training App. The objective of this short sprint was to proove the utility of machine learning, even in its nascent form. Within 45 minutes, everyone had a working "app" that recognized user positions and confirmed if they were correct yoga poses.</p>
        
        <p>To accomplish this task, which might have taken several days or weeks to mock-up, we simply had to train an adversarial neural network (check) on various poses. With the model trained in Wekinator, we could send an OSC signal over to Processing which was used toprovide minimal feedback to the user. The takeway from Day 1, was clear: machine learning is an eminently viable tool for designers. Even in its infancy, it can be used to speed up prototyping while also opening doors to voice and image detection.</p>
        
        <figure>
          <a href="../../../../../images/place-holder/1920x1080.jpg" title="Illustration of Yoga prototype." target="_blank">
            <img class="fit" src="../../../../../images/place-holder/1920x1080.jpg" style="background: rgb(240, 240, 240);">
          </a>
          <figcaption>
            <p class="small">Training Wekinator on Yoga poses.</p>
          </figcaption>
        </figure>
        
        <h2 class="mb">Day 2</h2>
        
        <p>The second day consisted of a more theoretical approach to machine learning. Gene took the class through high-level explanations of how computers can be taught. In doing so, he went through several of his projects to show various instances of machine learning in action. Before lunchtime, CIID alum and physical computing professor, <a href="http://bjoernkarmann.dk/">Bjørn Karmann</a> came in and showed us how he used machine learning in <a href="http://bjoernkarmann.dk/objectifier">Objectifier</a>, a small AC adapter for making dumb products smart. The point of his experiment was to explore how machine learning could be manifested in the physical world.</p>
        
        <p>After lunch, we were introduced to <a href="https://runwayml.com/">Runway</a>, a powerful client-side application for machine learning designed by <a href="https://cvalenzuelab.com/">Cris Valenzuela</a>. With HTTP, OSC, and camera input, it is quite powerful. Using <a href="https://www.docker.com/">Docker</a>, we were able to download several pretrained models including im2txt, OpenPose, and YOLO. Yet again, a world of interaction design which had once been religated to advanced developers was now made accesible to anybody with a little bit of curiosity.</p>
        
        <p>We finished out the day by returing to the concept which we had come across on the first day: "It is an [x] that you can control with [y]". For the rest of the day, we explored this concept, using a novel input to control a novel output. What I ended up with was a <a href="http://formandcode.com/code-examples/parameterize-chair">parametric chair</a> controlled via my gob.</p>
        
        <figure>
          <a href="./chair-face.mp4" title="Using facial expressions to control a chair's design." target="_blank">
            <video class="fit" style="background: rgb(123, 123, 123);" loop autoplay muted>
              <source src="./chair-face.mp4"  type="video/mp4">
            </video>
          </a>
          <figcaption>
            <p class="small">Facial tracking was done in openFrameworks and sent to Wekinator over OSC. Wekinator was trained on a continuous model. Data was then sent to Processsing to control the sketch.</p>
          </figcaption>
        </figure>
        
        <h2 class="mb">Days 3-5</h2>
        
        <p>With two days of training, we were ready to embark upon our first machine learning project. To start the project off right, Andreas had us break into small groups and do a round of 20/10/5 brainstorming (20 sketchs + doodles, in 10 minutes, with 5 minutes to share). From there, we had to come up with three more ideas in five minutes. After selecting our two favorite ideas from the group, we came together as a class and shared. From there, we voted on the best ideas and broke into groups based on which idea we wanted to bring to life.</p>
        
        <p>I ended up picking my own idea to prototype. I had come up with the idea after seeing the ridiculous captions that im2txt spit out. I was curious what would happen if you googled the caption and fed the first googled image back into the model. This idea was somewhat similar to Jonathan Chomko's <a href="http://jonathanchomko.com/News-Machine">News Machine</a> however, instead of relying on countless technologies to interpret a given signal, this project would depend entirely on machine learning.</p>
        
        <figure>
          <div class="flex">
            <a href="./brain-storming.jpg" title="Lars and Mikkel brainstorming." target="_blank">
              <img src="./brain-storming.jpg" style="background: rgb(211, 211, 211);">
            </a>
            <a href="./sticky-note.jpg" title="A pink sticky note." target="_blank">
              <img src="./sticky-note.jpg" style="background: rgb(255, 39, 122);">
            </a>
          </div>
          <figcaption class="flex">
            <p class="small">Lars and Mikkel brainstorming.</p>
            <p class="small">Goofy Google in its embryonic form.</p>
          </figcaption>
        </figure>
        
        <p>The following 48 hours were spent hacking through Runway data, web sockets, Google API documentation, and cross origin-errors. Thankfully, I was able to get a viable demo working for Friday. At 1:30 PM, I presented what I dubbed <a href="#">Goofy Google</a> to the entirerty of the second week CIID Summer School cohort. To end the week of school, we all went on a canal tour of Copenhagen. It was there that I got the chance to talk with Bjørn who suggested I make Goofy Google into a poster series or a short video montage.</p>
        
        <p>The project as a whole is complete, but it is in need of some clean up before it is ready to live on the web. For more information about the project itself, you can visit the <a href="#">breakdown page</a> or you can check out the <a href="#">source code</a> on GitHub. If you have any questions about my process or the class, or need help setting up something similar feel free to reach out to me.</p>
        
        <figure>
          <a href="./goofy-google.mp4" title="Google images and machine learning." target="_blank">
            <video class="fit" style="background: rgb(253, 196, 139);" loop autoplay muted>
              <source src="./goofy-google.mp4"  type="video/mp4">
            </video>
          </a>
          <figcaption>
            <p class="small">An infinte loop of machine learning.</p>
          </figcaption>
        </figure>
        
        <p class="italic">A special thanks goes out to Gene and Andreas for sharing their knowledge, to CIID for organizing the Summer School experience, and to Cris Valenzuela for having such beautifully documented code.</p>
        
      </article>

    </main>

    <footer class="wrap site__footer">
      <div>© 2018 Luka Schulz</div>
    </footer>

  </body>



</html>
